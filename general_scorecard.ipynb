{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scorecardpy as sc\n",
    "import pprint \n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.read_csv('IS453 Group Assignment - Data.csv')\n",
    "\n",
    "#use a copy of hmeq_data for credit risk model\n",
    "full_data = full_data.copy()\n",
    "\n",
    "full_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data['OCCUPATION_TYPE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_set = full_data.copy()\n",
    "working_set_orig = full_data.copy()\n",
    "\n",
    "working_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_set['FLAG_DOCUMENT_2'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_set.describe(include = 'all')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preparation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine which variables are continuous variables \n",
    "cont_variables = working_set[[\"STATUS\", \"AMT_INCOME_TOTAL\", \"AMT_CREDIT\", \"AMT_ANNUITY\", \"AMT_GOODS_PRICE\", \"REGION_POPULATION_RELATIVE\", \"EXT_SOURCE_1\", \"EXT_SOURCE_2\", \"EXT_SOURCE_3\", \"APARTMENTS_AVG\"]]\n",
    "\n",
    "#Check for highly correlated variables for CONTINUOUS VARIABLES ONLY \n",
    "cor = cont_variables.corr() \n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "sns.heatmap(cor,xticklabels=cor.columns,yticklabels=cor.columns,annot=True, ax=ax)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping Variables that result in biasedness and highly correlated ones\n",
    "working_set = working_set.drop(columns=['CODE_GENDER', 'AMT_ANNUITY', 'AMT_GOODS_PRICE'], axis=1)\n",
    "\n",
    "working_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect rows missing more than 4 values &  make sure not to reduce sample size too much \n",
    "rows_w_gt_3_na = working_set[working_set.isnull().sum(axis=1) > 3].shape[0]\n",
    "print('Percent of total rows missing more than 3 values: ' + str(\"{:.1%}\".format(rows_w_gt_3_na/working_set.shape[0])))\n",
    "print(rows_w_gt_3_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_set.dropna(thresh=working_set.shape[1]-3,inplace=True)\n",
    "print(\"Starting row count: \" + str(working_set_orig.shape[0]))\n",
    "print(\"Ending row count: \" + str(working_set.shape[0]))\n",
    "print(\"Percent dropped: \" + \"{:.2%}\".format(1-working_set.shape[0]/working_set_orig.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check % of missing column values again with revised dataset \n",
    "col_w_na = pd.DataFrame((working_set.isnull().sum().sort_values(ascending=False)/working_set.shape[0])).applymap(\"{0:.0%}\".format)\n",
    "\n",
    "col_w_na.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove 3 variables as the amount of missing data is more than 50% \n",
    "working_set = working_set.drop(columns=['OWN_CAR_AGE','EXT_SOURCE_1'], axis=1)\n",
    "\n",
    "working_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_set['FLAG_DOCUMENT_2'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Extraction -- Combinding the Flag Documents columns \n",
    "working_set['TOTAL_FLAG_DOCUMENTS'] = working_set['FLAG_DOCUMENT_2'] + working_set['FLAG_DOCUMENT_3'] + working_set['FLAG_DOCUMENT_4'] + working_set['FLAG_DOCUMENT_5'] + working_set['FLAG_DOCUMENT_6'] + working_set['FLAG_DOCUMENT_7'] + working_set['FLAG_DOCUMENT_8'] + working_set['FLAG_DOCUMENT_9'] + working_set['FLAG_DOCUMENT_10'] + working_set['FLAG_DOCUMENT_11'] + working_set['FLAG_DOCUMENT_12'] + working_set['FLAG_DOCUMENT_13'] + working_set['FLAG_DOCUMENT_14'] + working_set['FLAG_DOCUMENT_15'] + working_set['FLAG_DOCUMENT_16'] + working_set['FLAG_DOCUMENT_17'] + working_set['FLAG_DOCUMENT_18'] + working_set['FLAG_DOCUMENT_19'] + working_set['FLAG_DOCUMENT_20'] + working_set['FLAG_DOCUMENT_21']\n",
    "\n",
    "working_set['TOTAL_FLAG_DOCUMENTS'].head()\n",
    "\n",
    "working_set.drop('FLAG_DOCUMENT_2', inplace=True, axis=1)\n",
    "working_set.drop('FLAG_DOCUMENT_3', inplace=True, axis=1)\n",
    "working_set.drop('FLAG_DOCUMENT_4', inplace=True, axis=1)\n",
    "working_set.drop('FLAG_DOCUMENT_5', inplace=True, axis=1)\n",
    "working_set.drop('FLAG_DOCUMENT_6', inplace=True, axis=1)\n",
    "working_set.drop('FLAG_DOCUMENT_7', inplace=True, axis=1)\n",
    "working_set.drop('FLAG_DOCUMENT_8', inplace=True, axis=1)\n",
    "working_set.drop('FLAG_DOCUMENT_9', inplace=True, axis=1)\n",
    "working_set.drop('FLAG_DOCUMENT_10', inplace=True, axis=1)\n",
    "working_set.drop('FLAG_DOCUMENT_11', inplace=True, axis=1)\n",
    "working_set.drop('FLAG_DOCUMENT_12', inplace=True, axis=1)\n",
    "working_set.drop('FLAG_DOCUMENT_13', inplace=True, axis=1)\n",
    "working_set.drop('FLAG_DOCUMENT_14', inplace=True, axis=1)\n",
    "working_set.drop('FLAG_DOCUMENT_15', inplace=True, axis=1)\n",
    "working_set.drop('FLAG_DOCUMENT_16', inplace=True, axis=1)\n",
    "working_set.drop('FLAG_DOCUMENT_17', inplace=True, axis=1)\n",
    "working_set.drop('FLAG_DOCUMENT_18', inplace=True, axis=1)\n",
    "working_set.drop('FLAG_DOCUMENT_19', inplace=True, axis=1)\n",
    "working_set.drop('FLAG_DOCUMENT_20', inplace=True, axis=1)\n",
    "working_set.drop('FLAG_DOCUMENT_21', inplace=True, axis=1)\n",
    "\n",
    "working_set.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove missing values \n",
    "working_set.dropna(inplace=True)\n",
    "working_set.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate WOE Bins \n",
    "\n",
    "# automatically calculate bin ranges \n",
    "bins = sc.woebin(working_set, y='STATUS')\n",
    "\n",
    "# make it easy to read the bins\n",
    "for variables , bindetails in bins.items():\n",
    "    print(variables , \" : \")\n",
    "    display(bindetails)\n",
    "    print(\"--\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train-test split \n",
    "train, test = sc.split_df(working_set, 'STATUS', ratio=0.7).values()\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a dataset with the WOE values for Logistic Regression training\n",
    "# woebin_ply() converts original values of input data into woe\n",
    "train_woe = sc.woebin_ply(train, bins)\n",
    "test_woe = sc.woebin_ply(test, bins)\n",
    "train_woe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the X, y parts of data for train and test\n",
    "y_train = train_woe.loc[:,'STATUS']\n",
    "X_train = train_woe.loc[:,train_woe.columns != 'STATUS']\n",
    "y_test = test_woe.loc[:,'STATUS']\n",
    "X_test = test_woe.loc[:,train_woe.columns != 'STATUS']\n",
    "\n",
    "#create a logistic regression model object\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"beta coefficients:\")\n",
    "print(lr.coef_)\n",
    "print(\"alpha\")\n",
    "print(lr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a card from the model and bins\n",
    "card = sc.scorecard(bins, lr, X_train.columns, points0 = 600, odds0 = 1/20, pdo = 20,\n",
    "      basepoints_eq0 = True)\n",
    "\n",
    "pprint.pprint(card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the score card for occupation_type \n",
    "#See if drivers are discriminated against "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit score for samples in test and train\n",
    "train_score = sc.scorecard_ply(train, card)\n",
    "test_score = sc.scorecard_ply(test, card)\n",
    "\n",
    "#distrubution of scores on test data\n",
    "train_score.hist(figsize=(7,5),bins=60)\n",
    "plt.title('train data scores')\n",
    "test_score.hist(figsize=(7,5),bins=60)\n",
    "plt.title('test data scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print evaluation metrics of the model\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_pred, y_test))\n",
    "print(\"\\nPCC measures:\")\n",
    "print(classification_report(y_pred, y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
